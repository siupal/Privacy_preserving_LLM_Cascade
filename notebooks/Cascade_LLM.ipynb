{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "height": 177,
     "referenced_widgets": [
      "1be5de668f4b469686aa7dcea6e7c1e2",
      "269d23517eaf4ea788cb1905d087e245",
      "c85c9115be104f638d0f8a3509eeb7af",
      "2e8dabd840714dc5aa6430d4cc6b0183",
      "4f43a9997cb4485fae12a3c735a27098",
      "e19ccaeba1e2440e829c5c7b93d2ac23",
      "1ea7f4804a2c45c9bb5395a3e4cd3c2f",
      "a21b4a600bc24402a927e247088401c7",
      "bd259eeb05fb4915b5294279770f605d",
      "d0b9b813e45e4dbf8ccf9a36f8777a53",
      "5fee170c00434a1396169c5e55356ca1"
     ]
    },
    "executionInfo": {
     "elapsed": 24102,
     "status": "ok",
     "timestamp": 1725991869988,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "Fl0-vQ19PBv4",
    "outputId": "dc689fc6-741a-47ce-c830-0b480647e3f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.3453488504364.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.3453488504364.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/transformers/utils/generic.py:339: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.3453488504364.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/transformers/utils/generic.py:339: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be5de668f4b469686aa7dcea6e7c1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tempfile\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# cns_path_llama=r\"/cns/iq-d/home/kkaizh/Gemma/gemma-1.1-2b-it\"\n",
    "\n",
    "from google3.pyglib import gfile\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "CELL = 'iq'\n",
    "USER = 'kkaizh'\n",
    "REPO= 'Gemma/gemma-1.1-2b-it' # option:Gemma/gemma-1.1-2b-it, llama/llama-7b\n",
    "cache_dir = tempfile.gettempdir()\n",
    "model_repo = f'/cns/{CELL}-d/home/{USER}/{REPO}'\n",
    "\n",
    "# test loading from checkpoint\n",
    "ckp_path=r\"/cns/iq-d/home/kkaizh/multi_obj_cascade_llm/instruction_tuning/checkpoint-500\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_repo, cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_repo, cache_dir=cache_dir).to(device)\n",
    "\n",
    "generation_config = transformers.GenerationConfig(\n",
    "        max_length=2048,\n",
    "        renomalize_logits=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        output_logits=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1725046613488,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "XFdPZOyxylR6",
    "outputId": "3c586f2d-21c4-44d4-a9cd-50879ada17b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 9560.2890625 MB\n"
     ]
    }
   ],
   "source": [
    "num_parameters = model.num_parameters()\n",
    "total_size = num_parameters * 4  # Assuming 4 bytes per float32 parameter\n",
    "print(\"Model size:\", total_size / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al1i9GfqRBcm"
   },
   "source": [
    "# Ligots-based generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1724103351684,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "K8tvMEKKu3x3",
    "outputId": "e3e3ca1d-73f0-41c5-87ec-30786c819fdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 👋\n",
      "\n",
      "Hello! 👋 It's great to hear from you. How can I assist you today? 😊<eos>\n",
      "0.8459854\n",
      " -0.491 | 61.19%\n",
      " -0.642 | 52.62%\n",
      " -0.152 | 85.88%\n",
      " -0.020 | 98.07%\n",
      " -0.505 | 60.38%\n",
      " -0.017 | 98.31%\n",
      " -0.378 | 68.53%\n",
      " -0.001 | 99.92%\n",
      " -0.000 | 100.00%\n",
      " -0.730 | 48.17%\n",
      " -0.001 | 99.94%\n",
      " -0.258 | 77.30%\n",
      " -0.000 | 100.00%\n",
      " -0.000 | 99.98%\n",
      " -0.009 | 99.06%\n",
      " -0.216 | 80.54%\n",
      " -0.097 | 90.75%\n",
      " -0.000 | 100.00%\n",
      " -0.540 | 58.25%\n",
      " -0.000 | 99.99%\n",
      " -0.000 | 99.95%\n",
      " -0.000 | 100.00%\n",
      " -0.663 | 51.55%\n",
      " -0.000 | 99.99%\n"
     ]
    }
   ],
   "source": [
    "# generation test\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "input_test = tokenizer(\"This is a test, please say hello to me\", return_tensors=\"pt\", max_length=128)\n",
    "output = model.generate(input_test[\"input_ids\"].to(device), generation_config=generation_config)\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    output.sequences, output.scores, normalize_logits=True\n",
    ")\n",
    "# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
    "# encoder-decoder models, like BART or T5.\n",
    "input_length = 1 if model.config.is_encoder_decoder else input_test.input_ids.shape[1]\n",
    "generated_tokens = output.sequences[:, input_length:]\n",
    "print(tokenizer.decode(generated_tokens[0]))\n",
    "print(np.mean(np.exp(transition_scores.cpu().numpy())))\n",
    "for score in transition_scores[0]:\n",
    "    # | token | token string | log probability | probability\n",
    "    print(f\" {score.cpu().numpy():.3f} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0kfT-kdRKoK"
   },
   "source": [
    "# GSM8K dataset experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1725043540100,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "Xly4K94A3B4v",
    "outputId": "633e091f-d9e7-4a87-accf-54184550cef6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8837341\n",
      "143\n",
      "The answer is 45<eos>\n"
     ]
    }
   ],
   "source": [
    "# problem solving test\n",
    "prompt_test = r'''Assume you're a student working on some mathematical problems. Now, you'll be giving mathematical problems, you need to do two tasks: a. Check if the question contains personal information (e.g., names etc.), output Yes or No only;\\n\n",
    "b. Solve this question; \\n\n",
    "\n",
    "Herer are some examples:\n",
    "Question: Hector purchased a container of gumballs. He gave 4 to Todd, then he gave twice as many as he had given Todd to Alisha, and then he gave 5 less than four times as many to Bobby as he had given to Alisha. If Hector had 6 gumballs remaining, what is the total number of gumballs that Hector purchased? \\n\n",
    "Output:\n",
    "a. Contains Personal Information: Yes\n",
    "b. Answer: Hector gave to Alisha twice as many as he had given Todd, for a total of 4*2=<<4*2=8>>8 gumballs, Hector gave 5 less than four times as many to Bobby as he had given to Alisha, or a total of (8*4)-5=<<8*4-5=27>>27 gumballs. If Hector had 6 gumballs remaining, he originally purchased 4+8+27+6=<<4+8+27+6=45>>45 gumballs. #### 45\n",
    "\n",
    "Case:\n",
    "Auestion: Hector purchased a container of gumballs. He gave 4 to Todd, then he gave twice as many as he had given Todd to Alisha, and then he gave 5 less than four times as many to Bobby as he had given to Alisha. If Hector had 6 gumballs remaining, what is the total number of gumballs that Hector purchased?\\n\n",
    "Output:'''\n",
    "test_ = r'''\n",
    "a. Contains Personal Information: Yes\n",
    "b. Answer: Hector gave to Alisha twice as many as he had given Todd, for a total of 4*2=<<4*2=8>>8 gumballs, Hector gave 5 less than four times as many to Bobby as he had given to Alisha, or a total of (8*4)-5=<<8*4-5=27>>27 gumballs. If Hector had 6 gumballs remaining, he originally purchased 4+8+27+6=<<4+8+27+6=45>>45 gumballs. #### 45\n",
    "'''\n",
    "tokenized_prompt_test = tokenizer(test_, return_tensors=\"pt\", max_length=1024)\n",
    "prompt_len = 1 if model.config.is_encoder_decoder else tokenized_prompt_test.input_ids.shape[1]\n",
    "output_answer = model.generate(tokenized_prompt_test[\"input_ids\"].to(device),generation_config=generation_config)\n",
    "generated_answer_tokens = output_answer.sequences[:, prompt_len:]\n",
    "decoded_str = tokenizer.decode(generated_answer_tokens[0])\n",
    "transition_scores_ = model.compute_transition_scores(\n",
    "    output_answer.sequences, output_answer.scores, normalize_logits=True\n",
    ")\n",
    "print(np.mean(np.exp(transition_scores_.cpu().numpy())))\n",
    "print(prompt_len)\n",
    "print(decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irZNG0iT3mkz"
   },
   "outputs": [],
   "source": [
    "# self-critque test\n",
    "# self_check_prompt = r'''Assume you're a scorer. Given a correct solution: {correct_answer}\\n\n",
    "# and a solution by a student: {generated_ans}\\n\n",
    "# Please check if the student's solution is correct or not'''\n",
    "# generated_answer = decoded_str\n",
    "# golden_answer = \"It rained 3+4=<<3+4=7>>7 inches So he collected 7*15=<<7*15=105>>105 gallons So he makes 105*1.2=$<<105*1.2=126>>126 from selling the water #### 126\"\n",
    "# self_check_test = self_check_prompt.format(correct_answer=golden_answer, generated_ans=generated_answer)\n",
    "# tokenized_self_check_test = tokenizer(self_check_test, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "# self_check_len = tokenized_self_check_test['input_ids'].shape[1]\n",
    "# output_answer = model.generate(tokenized_self_check_test[\"input_ids\"].to(device), max_length=1024)\n",
    "# print(self_check_len)\n",
    "# print(tokenizer.decode(output_answer[0][self_check_len:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1936,
     "status": "ok",
     "timestamp": 1723760559464,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "F-1oy3KPfLnN",
    "outputId": "34120506-4b4a-4e70-b720-d7bf9b04486f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956\n",
      "\n",
      "a. Contains Personal Information: Yes\n",
      "b. Answer: Irene earned $500 for 40 hours, and an extra $20 for each hour of overtime. Therefore, Irene earned $500 + 20*5 = $500 + $100 = $600 for 50 hours of work.\n",
      "c. Confidence Level: Moderate\n",
      "d. Rewritten question: No<eos>\n"
     ]
    }
   ],
   "source": [
    "# prompt tuning\n",
    "prompt = \"\"\"Assume you're a student working on some mathematical problems. Now, you'll be giving mathematical problems, you need to do four tasks: a. Check if the question contains personal information (e.g., names etc.), output \"Contains personal information: Yes or No only\";\\n\n",
    "b. Solve this question; \\n\n",
    "c. Do a self-critique on your answer quality, output your answer and your confidence level (low, moderate, high);\\n\n",
    "d. Identify if the question needs to be rewritten. If contains personal information and the confidence level is not high then output \"Rewritten question: Yes\" else output \"Rewritten question: No\";\\n\n",
    "\n",
    "Herer are some examples that teach you when to rewrite (answer confidence level is low/moderate and question contains personal information):\n",
    "Question: Hector purchased a container of gumballs. He gave 4 to Todd, then he gave twice as many as he had given Todd to Alisha, and then he gave 5 less than four times as many to Bobby as he had given to Alisha. If Hector had 6 gumballs remaining, what is the total number of gumballs that Hector purchased? \\n\n",
    "Output:\n",
    "a. Contains Personal Information: Yes\n",
    "b. Answer: Hector gave to Alisha twice as many as he had given Todd, for a total of 4*2=<<4*2=8>>8 gumballs, Hector gave 5 less than four times as many to Bobby as he had given to Alisha, or a total of (8*4)-5=<<8*4-5=27>>27 gumballs. If Hector had 6 gumballs remaining, he originally purchased 4+8+27+6=<<4+8+27+6=45>>45 gumballs. #### 45.\n",
    "c. Confidence Level: High\n",
    "d. Rewritten question: No.\n",
    "\n",
    "Question: A garden produced 237 potatoes, 60 fewer cucumbers and twice as many peppers than the cucumbers. How many vegetables did the garden produce? \\n\n",
    "Output:\n",
    "a. Contains Personal Information: No\n",
    "b. Answer: 237 potatoes + 60 cucumbers + 2*60 peppers = 237 + 60 + 120 = 317 vegetables.<eos>.\n",
    "c. Confidence Level: Moderate\n",
    "d. Rewritten question: No\n",
    "\n",
    "Question: A boxer weighs 97 kg at 4 months from a fight. He is on a diet that allows him to lose 3 kg per month until the day of the fight. How much will he weigh on the day of the fight?\n",
    "Output:\n",
    "a. Contains Personal Information: No\n",
    "b. Answer: 97 kg - 3 kg/month * 4 months = 97 kg.<eos>\n",
    "c. Confidence Level: Low\n",
    "d. Rewritten question: No\n",
    "\n",
    "Question: Krystian works in the library. He borrows an average of 40 books every day. Every Friday, his number of borrowed books is about 40% higher than the daily average. How many books does he borrow in a week if the library is open from Monday to Friday? \\n\n",
    "Output:\n",
    "a. Contains Personal Information: Yes\n",
    "b. Answer: 40% higher than the daily average means 40% of 40 = 16 books. So Krystian borrows 40 + 16 = 56 books in a week.<eos>\n",
    "c. Confidence Level: Low\n",
    "d. Rewritten question: Yes\n",
    "\n",
    "Question: Sally and Bob have made plans to go on a trip at the end of the year. They both decide to work as babysitters and save half of what they've earned for their trip. If Sally makes $6 per day and Bob makes $4 per day, how much money will they both have saved for their trip after a year? \\n\n",
    "Output:\n",
    "a. Contains Personal Information: Yes\n",
    "b. Answer: Sally and Bob will have saved $6*365=<<$6*365=22,150>>22,150 for their trip after a year.<eos>\n",
    "c. Confidence Level: Moderate\n",
    "d. Rewritten question: Yes\n",
    "\n",
    "Your turn:\n",
    "Auestion: {question}\\n\n",
    "Output:\"\"\"\n",
    "question = \"Irene earns $500 if she works for 40 hours a week and gets an extra $20 for every hour of overtime. If she worked 50 hours last week, calculate her total income.\"\n",
    "input_ = tokenizer(prompt.format(question=question), return_tensors=\"pt\", truncation=True, max_length=1536)\n",
    "prompt_length = input_['input_ids'].shape[1]\n",
    "print(prompt_length)\n",
    "output_ = model.generate(input_[\"input_ids\"].to(device), max_length=2048)\n",
    "print(tokenizer.decode(output_[0][prompt_length:]))\n",
    "# Two tasks - One is private or not / One is hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE5aph8UqTsk"
   },
   "outputs": [],
   "source": [
    "raw_train_data = []\n",
    "with gfile.Open(os.path.join(\"/cns/iq-d/home/kkaizh/multi_obj_cascade_llm/datasets/GSM8K/train.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n",
    "  # raw_train_data = [json.loads(line) for line in f]\n",
    "  for line in f:\n",
    "    tmp = json.loads(line)\n",
    "    tmp_data = {\"input\": tmp[\"question\"], \"label\": tmp[\"answer\"]}\n",
    "    raw_train_data.append(tmp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1724282384824,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "OVjoVloxuJ_o",
    "outputId": "dd7fdd7f-e3c3-4b6e-aea1-fc3768a0db4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'output': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1724281970747,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "_y6jSL5mNQnY",
    "outputId": "cf5e053e-3b90-4372-8678-b8aaef2631ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'answer'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wOrQ9DbRbYH"
   },
   "source": [
    "# WMT22 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1725835249878,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "0Ylf3pStRe6p"
   },
   "outputs": [],
   "source": [
    "meta_path=r\"/cns/iq-d/home/kkaizh/multi_obj_cascade_llm/datasets/wmt22/\"\n",
    "file_name = [\"europarl-v10.cs-en.pair.tsv\", \"europarl-v10.fr-en.pair.tsv\", \"europarl-v10.de-en.pair.tsv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 215988,
     "status": "ok",
     "timestamp": 1725835468804,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "BBIZYyvgSjzq",
    "outputId": "3bad57a7-5d04-4033-cc4a-cd3c25c0c741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645330\n",
      "1915930\n",
      "1828521\n"
     ]
    }
   ],
   "source": [
    "# preprocess raw data\n",
    "def process_raw(path):\n",
    "  with gfile.Open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "      _=line.split(\"\\t\")[:2]\n",
    "      tmp={\"src\": _[0], \"target\": _[1]}\n",
    "      data.append(tmp)\n",
    "  return data\n",
    "\n",
    "cs_en=process_raw(os.path.join(meta_path, file_name[0]))\n",
    "fr_en=process_raw(os.path.join(meta_path, file_name[1]))\n",
    "de_en=process_raw(os.path.join(meta_path, file_name[2]))\n",
    "\n",
    "print(len(cs_en))\n",
    "print(len(fr_en))\n",
    "print(len(de_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1725578905379,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "viV6C7zcgK8o",
    "outputId": "96be0e89-8bf6-4dde-dd5b-c239c3378bc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': 'To znamená, že co se mne týče, mám ohledně samotného principu mechanismu, jehož návrh máme před sebou, velké nejasnosti a pochyby.', 'target': 'This means that I, for my part, remain very puzzled and very doubtful as to the very principle of the mechanism that is proposed to us.'}\n"
     ]
    }
   ],
   "source": [
    "print(cs_en[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 145108,
     "status": "ok",
     "timestamp": 1725836742481,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "gBrvRkJlJQkn"
   },
   "outputs": [],
   "source": [
    "with gfile.Open(os.path.join(meta_path, \"cs_en.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(cs_en, f)\n",
    "with gfile.Open(os.path.join(meta_path, \"fr_en.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(fr_en, f)\n",
    "with gfile.Open(os.path.join(meta_path, \"de_en.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(de_en, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3704,
     "status": "ok",
     "timestamp": 1725839993355,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "qXmBHn_VE_KF",
    "outputId": "4482bb0e-3376-4198-9864-b3d3c142f5ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': '4.', 'target': '4.'}\n"
     ]
    }
   ],
   "source": [
    "with gfile.Open(os.path.join(meta_path, \"cs_en.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "  test = json.load(f)\n",
    "\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUbRqtKgJrQh"
   },
   "outputs": [],
   "source": [
    "prompt_translate=r'''\n",
    "Assume you're a professional translator, now please translate the following sentence into English. Output the translated sentence only.\\n\n",
    "Source sentence: {src}\\n\n",
    "Target sentence:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1329,
     "status": "ok",
     "timestamp": 1724887557588,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "DlQae8VRKnxx",
    "outputId": "a85f8c7d-7239-4a75-ff27-fd50d6cc218c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      ">The secondledem, which was decisive for my message, is that, as evidenced by the implementation of the Hague Program for the year 2007, the level of implementation, particularly in cooperation on criminal justice matters, was lower than in other areas, such as civil cooperation, border management, and immigration and asylum policies, which yielded positive results.<eos>\n",
      "The second consideration on which I based my report is that, as may be seen from the report on the implementation of the Hague Programme for 2007, the level of implementation with regard to legal cooperation in the criminal field was somewhat low, even though satisfactory developments were registered in other sectors, such as civil co-operation, border management, immigration and asylum policies.\n"
     ]
    }
   ],
   "source": [
    "input_translate=tokenizer(prompt_translate.format(src=cs_en[66][\"src\"]), return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "translate_length=input_translate['input_ids'].shape[1]\n",
    "output_translate=model.generate(input_translate[\"input_ids\"].to(device), max_length=512)\n",
    "print(translate_length)\n",
    "print(tokenizer.decode(output_translate[0][translate_length:]))\n",
    "print(cs_en[66][\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgn860hiYMOb"
   },
   "source": [
    "# MeQSum Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwbWDnRTYLGi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "meqsum_path=r\"/cns/iq-d/home/kkaizh/multi_obj_cascade_llm/datasets/MedQSum/MeQSum.csv\"\n",
    "meqsum = pd.read_csv(meqsum_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 206
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1725576239160,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "-S5CURfDYLV1",
    "outputId": "9f6990d3-1363-4288-b448-8ba6f0dfed04"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"meqsum\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"File\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"16266.txt\",\n          \"66.txt\",\n          \"NF_86.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CHQ\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"SUBJECT: new treatment for lipoma of forearm\\nMESSAGE: I have had two surgeries on my right forearm.  The last was 2 years ago.  The lipoma has returned.  Are there any new treatments or experimental procedures that might help me?\",\n          \"Beckwith-Wieddeman Syndrome. I would like to request further knowledge on this specific disorder.\",\n          \"SUBJECT: MedlinePlus Service Request\\nMESSAGE: if A PERSON HAS BEEN USING MORPHINE FOR YEARS (10)FOR PAIN RELIEF,AND ARE EXHIBITING THE FLU-LIKE INABILITY TO THINK OR COMMUNICATE, AS A LOVING FRIEND , COULD YOU GUESS/ESTIMATE FOR ME HOW LONG THE HELL THEY'RE EXPERIENCING MAY LAST?1 WEEK, 1 YEAR?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 994,\n        \"samples\": [\n          \"How is a differential diagnosis made between SLE and adult still's disease?\",\n          \"What are the causes of loss of ear pressure and can it heal itself?\",\n          \"What causes fetal diaphragmatic hernia, and what can be done to prevent it?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "meqsum"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6b8e57f9-3e2f-4a1d-a0cc-b7a07ee0e6dc\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>CHQ</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-131188152.xml.txt</td>\n",
       "      <td>SUBJECT: who and where to get cetirizine - D\\n...</td>\n",
       "      <td>Who manufactures cetirizine?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14348.txt</td>\n",
       "      <td>who makes bromocriptine\\ni am wondering what c...</td>\n",
       "      <td>Who manufactures bromocriptine?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-131985747.xml.txt</td>\n",
       "      <td>SUBJECT: nulytely\\nMESSAGE: Hello can you tell...</td>\n",
       "      <td>Who makes nulytely, and where can I buy it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15410.txt</td>\n",
       "      <td>Williams' syndrome\\nI would like to have my da...</td>\n",
       "      <td>Where can I get genetic testing for william's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.txt</td>\n",
       "      <td>ClinicalTrials.gov - Question - general inform...</td>\n",
       "      <td>Where can I get genetic testing for multiple m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b8e57f9-3e2f-4a1d-a0cc-b7a07ee0e6dc')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6b8e57f9-3e2f-4a1d-a0cc-b7a07ee0e6dc button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6b8e57f9-3e2f-4a1d-a0cc-b7a07ee0e6dc');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-60666377-b383-4bd9-8a13-9d6c521b22ea\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-60666377-b383-4bd9-8a13-9d6c521b22ea')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-60666377-b383-4bd9-8a13-9d6c521b22ea button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                  File  ...                                            Summary\n",
       "0  1-131188152.xml.txt  ...                       Who manufactures cetirizine?\n",
       "1            14348.txt  ...                    Who manufactures bromocriptine?\n",
       "2  1-131985747.xml.txt  ...        Who makes nulytely, and where can I buy it?\n",
       "3            15410.txt  ...  Where can I get genetic testing for william's ...\n",
       "4               35.txt  ...  Where can I get genetic testing for multiple m...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meqsum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1725576243149,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "escmdro-Z6N_",
    "outputId": "9f8b0971-a974-4478-8f5c-b6ea313f7f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CHQ of 0-th data is:SUBJECT: who and where to get cetirizine - D\n",
      "MESSAGE: I need/want to know who manufscturs Cetirizine. My Walmart is looking for a new supply and are not getting the recent\n",
      "The Summary of 0-th data is:Who manufactures cetirizine?\n"
     ]
    }
   ],
   "source": [
    "print(\"The CHQ of 0-th data is:{0}\".format(meqsum[\"CHQ\"][0]))\n",
    "print(\"The Summary of 0-th data is:{0}\".format(meqsum[\"Summary\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cB9FVrLZ60r"
   },
   "outputs": [],
   "source": [
    "# meqsum_prompt = r'''\n",
    "# Please summarize the below consumer health question (CHQ) following the given examples, output the summarized question only:\\n\n",
    "\n",
    "# Examples:\n",
    "# CHQ: SUBJECT: who and where to get cetirizine - D\n",
    "# MESSAGE: I need/want to know who manufscturs Cetirizine. My Walmart is looking for a new supply and are not getting the recent\n",
    "# Summary: Who manufactures cetirizine?\n",
    "\n",
    "# CHQ: who makes bromocriptine\n",
    "# i am wondering what company makes the drug bromocriptine, i need it for a mass i have on my pituitary gland and the cost just keeps raising. i cannot ever buy a full prescription because of the price and i was told if i get a hold of the maker of the drug sometimes they offer coupons or something to help me afford the medicine. if i buy 10 pills in which i have to take 2 times a day it costs me 78.00. and that is how i have to buy them.  thanks.\n",
    "# Summary: Who manufactures bromocriptine?\n",
    "\n",
    "# Case:\n",
    "# CHQ: {chq}\\n\n",
    "# Summary:\n",
    "# '''\n",
    "meqsum_prompt = r'''\n",
    "Please summarize the below consumer health question (CHQ) into a simple question, output the summarized question only.\n",
    "\n",
    "CHQ: {chq}\\n\n",
    "Summary:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1725503559426,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "WCcm_LW4b2fz",
    "outputId": "27e39967-33e2-44d8-bbc2-438075d3a6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8329723\n",
      "91\n",
      "The consumer is seeking information about commercial genetic tests for the IHHS heart condition in Texas.<eos>\n"
     ]
    }
   ],
   "source": [
    "tokenized_meqsum = tokenizer(meqsum_prompt.format(chq=meqsum[\"CHQ\"][5]), return_tensors=\"pt\", max_length=256)\n",
    "prompt_len = 1 if model.config.is_encoder_decoder else tokenized_meqsum.input_ids.shape[1]\n",
    "output_answer = model.generate(tokenized_meqsum[\"input_ids\"].to(device),generation_config=generation_config)\n",
    "generated_answer_tokens = output_answer.sequences[:, prompt_len:]\n",
    "decoded_str = tokenizer.decode(generated_answer_tokens[0])\n",
    "transition_scores_ = model.compute_transition_scores(\n",
    "    output_answer.sequences, output_answer.scores, normalize_logits=True\n",
    ")\n",
    "print(np.mean(np.exp(transition_scores_.cpu().numpy())))\n",
    "print(prompt_len)\n",
    "print(decoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFTt0hDBLw-v"
   },
   "outputs": [],
   "source": [
    "def obtain_response_logits(CHQ, meqsum_prompt):\n",
    "  tokenized_meqsum = tokenizer(meqsum_prompt.format(chq=CHQ), return_tensors=\"pt\", max_length=512)\n",
    "  prompt_len = 1 if model.config.is_encoder_decoder else tokenized_meqsum.input_ids.shape[1]\n",
    "  output_answer = model.generate(tokenized_meqsum[\"input_ids\"].to(device),generation_config=generation_config)\n",
    "  generated_answer_tokens = output_answer.sequences[:, prompt_len:]\n",
    "  decoded_str = tokenizer.decode(generated_answer_tokens[0])\n",
    "  transition_scores = model.compute_transition_scores(\n",
    "      output_answer.sequences, output_answer.scores, normalize_logits=True\n",
    "  )\n",
    "  raw_logits = np.exp(transition_scores.cpu().numpy())\n",
    "  mean_logits = np.mean(raw_logits)\n",
    "  median_logits = np.median(raw_logits)\n",
    "  quantile25 = np.quantile(raw_logits, 0.25)\n",
    "  quantile50 = np.quantile(raw_logits, 0.50)\n",
    "  quantile75 = np.quantile(raw_logits, 0.75)\n",
    "  logits = {\"mean\": str(mean_logits), \"median\": str(median_logits), \"quantile25\": str(quantile25), \"quantile50\": str(quantile50), \"quantile75\": str(quantile75)}\n",
    "\n",
    "  return {\"chq\": CHQ, \"generated_summary\": decoded_str, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1725503567168,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "fX2kBRCuUCbY",
    "outputId": "d0458a94-5b16-4b68-8a38-ed9c2add1dee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chq': 'Genetic Test for IHHS heart condition. Is there a commercial genetic test for the IHHS heart condition?  My family suffers from this heridity condition and I would like to know who is susseptable.   If so, where could I get it done in [LOCATION] Texas?',\n",
       " 'generated_summary': 'The consumer is seeking information about commercial genetic tests for the IHHS heart condition in Texas.<eos>',\n",
       " 'logits': {'mean': '0.8329723',\n",
       "  'median': '0.9106504',\n",
       "  'quantile25': '0.7449459433555603',\n",
       "  'quantile50': '0.910650372505188',\n",
       "  'quantile75': '0.9959591925144196'}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtain_response_logits(meqsum[\"CHQ\"][5], meqsum_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-MsCnHcVyvC"
   },
   "outputs": [],
   "source": [
    "meqsum_results = []\n",
    "for i in range(len(meqsum)):\n",
    "  meqsum_results.append(obtain_response_logits(meqsum[\"CHQ\"][i], meqsum_prompt))\n",
    "\n",
    "with gfile.Open(r\"/cns/iq-d/home/kkaizh/multi_obj_cascade_llm/datasets/MedQSum/meqsum_0shot.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(meqsum_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18HurB3gXDp4"
   },
   "source": [
    "## Obtain safety check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXVr6MhfX0ZD"
   },
   "outputs": [],
   "source": [
    "def obtain_safety_response(CHQ, safety_check_prompt):\n",
    "  tokenized_meqsum = tokenizer(safety_check_prompt.format(chq=CHQ), return_tensors=\"pt\", max_length=512)\n",
    "  prompt_len = 1 if model.config.is_encoder_decoder else tokenized_meqsum.input_ids.shape[1]\n",
    "  output_answer = model.generate(tokenized_meqsum[\"input_ids\"].to(device),generation_config=generation_config)\n",
    "  generated_answer_tokens = output_answer.sequences[:, prompt_len:]\n",
    "  decoded_str = tokenizer.decode(generated_answer_tokens[0])\n",
    "  if \"Yes\" in decoded_str:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fY7cDIKqXI5q"
   },
   "outputs": [],
   "source": [
    "# safety_check_prompt=r'''\n",
    "# DO you think there is privacy concern in the following consumer health question (anything involves personal information considered as privacy)? Answer Yes or No only. \\n\n",
    "# CHQ: {chq}\\n\n",
    "# '''\n",
    "\n",
    "safety_check_prompt=r'''\n",
    "DO you think there is privacy concern in the following consumer health question (anything involves personal information considered as privacy)? Answer Yes or No only. \\n\n",
    "\n",
    "Here are some examples:\n",
    "CHQ: SUBJECT: who and where to get cetirizine - D\n",
    "MESSAGE: I need/want to know who manufscturs Cetirizine. My Walmart is looking for a new supply and are not getting the recent\n",
    "Answer: No\n",
    "\n",
    "CHQ: SUBJECT: who makes bromocriptine\n",
    "MESSAGE: I am wondering what company makes the drug bromocriptine, i need it for a mass i have on my pituitary gland and the cost just keeps raising. i cannot ever buy a full prescription because of the price and i was told if i get a hold of the maker of the drug sometimes they offer coupons or something to help me afford the medicine. if i buy 10 pills in which i have to take 2 times a day it costs me 78.00. and that is how i have to buy them.  thanks.\n",
    "Answer: Yes\n",
    "\n",
    "CHQ: {chq}\\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj3G1ugAYUJB"
   },
   "outputs": [],
   "source": [
    "# load results\n",
    "with gfile.Open(r\"/cns/iq-d/home/kkaizh/multi_obj_cascade_llm/datasets/MedQSum/meqsum_0shot.json\", \"r\") as f:\n",
    "  meqsum_0shot = json.load(f)\n",
    "\n",
    "with gfile.Open(r\"/cns/iq-d/home/kkaizh/multi_obj_cascade_llm/datasets/MedQSum/meqsum_2shot.json\", \"r\") as f:\n",
    "  meqsum_2shot = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1725576861044,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "Z7iU8lHrZSLC",
    "outputId": "811dfa6c-27ab-4131-e111-147f90ebf6b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chq': 'SUBJECT: who and where to get cetirizine - D\\nMESSAGE: I need/want to know who manufscturs Cetirizine. My Walmart is looking for a new supply and are not getting the recent',\n",
       " 'generated_summary': '- Cetirizine is a common over-the-counter (OTC) medication used to treat allergies and cold symptoms.\\n- It is manufactured by multiple companies.\\n- Walmart is looking for a new supplier for Cetirizine.\\n\\nPlease provide a concise and understandable summary of the CHQ.<eos>',\n",
       " 'logits': {'mean': '0.8311022',\n",
       "  'median': '0.9056625',\n",
       "  'quantile25': '0.7018575668334961',\n",
       "  'quantile50': '0.905662477016449',\n",
       "  'quantile75': '0.9983581900596619'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meqsum_0shot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BL0q54pKYUL1"
   },
   "outputs": [],
   "source": [
    "for i in range(len(meqsum_0shot)):\n",
    "  meqsum_0shot[i][\"safety_check\"]=obtain_safety_response(meqsum_0shot[i][\"chq\"], safety_check_prompt)\n",
    "\n",
    "with gfile.Open(r\"/cns/iq-d/home/kkaizh/multi_obj_cascade_llm/datasets/MedQSum/meqsum_2shot_satefy_check_.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(meqsum_0shot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1725580520572,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "IHJy-6XwWB0D",
    "outputId": "0feb82f0-c471-45af-939e-94358706038a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n"
     ]
    }
   ],
   "source": [
    "safety_count = [point[\"safety_check\"] for point in meqsum_0shot]\n",
    "print(safety_count.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1725580527614,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "9ecaPobGnBuK",
    "outputId": "abfb1676-01c9-4408-c2a1-d77b616de2cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(safety_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bv3wJNHWRXxw"
   },
   "source": [
    "# Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JE-Y73XSnrdb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize(text):\n",
    "  \"\"\"\n",
    "  Simple tokenizer that splits text by whitespace.\n",
    "\n",
    "  Args:\n",
    "      text: A string to be tokenized.\n",
    "\n",
    "  Returns:\n",
    "      A list of tokens.\n",
    "  \"\"\"\n",
    "  return text.lower().split()\n",
    "\n",
    "def ngrams(sequence, n):\n",
    "  \"\"\"\n",
    "  Generates n-grams from a sequence.\n",
    "\n",
    "  Args:\n",
    "      sequence: A list of tokens.\n",
    "      n: The n-gram size.\n",
    "\n",
    "  Returns:\n",
    "      A list of n-grams (subsequences of length n).\n",
    "  \"\"\"\n",
    "  return [sequence[i:i+n] for i in range(len(sequence) - n + 1)]\n",
    "\n",
    "def rouge_score(hypotheses, references, n=1):\n",
    "  \"\"\"\n",
    "  Calculates basic ROUGE-N score (precision) between hypotheses and references.\n",
    "\n",
    "  Args:\n",
    "      hypotheses: A list of strings representing the hypothesis sequences.\n",
    "      references: A list of lists of strings representing the reference sequences\n",
    "                  for each hypothesis.\n",
    "      n: The n-gram size (default: 1 for ROUGE-1).\n",
    "\n",
    "  Returns:\n",
    "      A list of ROUGE-N scores for each hypothesis-reference pair.\n",
    "  \"\"\"\n",
    "\n",
    "  scores = []\n",
    "  for hyp, refs in zip(hypotheses, references):\n",
    "    hyp_ngrams = set(ngrams(tokenize(hyp), n))\n",
    "    ref_ngrams_count = np.zeros((len(refs), len(hyp_ngrams)))\n",
    "\n",
    "    for i, ref in enumerate(refs):\n",
    "      for ngram in ngrams(tokenize(ref), n):\n",
    "        if ngram in hyp_ngrams:\n",
    "          ref_ngrams_count[i, hyp_ngrams.index(ngram)] += 1\n",
    "\n",
    "    max_ref_ngrams = np.max(ref_ngrams_count, axis=0)\n",
    "    precision = np.mean([np.sum(ngrams) / max(len(ngrams), 1) for ngrams in ref_ngrams_count])\n",
    "    scores.append(precision)\n",
    "\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpxzuAPpu88V"
   },
   "outputs": [],
   "source": [
    "# Setup training arguments.\n",
    "def get_training_args(\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    weight_decay: float,\n",
    ") -> transformers.Seq2SeqTrainingArguments:\n",
    "  return transformers.Seq2SeqTrainingArguments(\n",
    "      warmup_steps=10,\n",
    "      logging_steps=100,\n",
    "      fp16=True,\n",
    "      learning_rate=learning_rate,\n",
    "      weight_decay=weight_decay,\n",
    "      num_train_epochs=num_epochs,\n",
    "      logging_dir='logs/',\n",
    "      output_dir='results/',\n",
    "      per_device_eval_batch_size=batch_size,\n",
    "      per_device_train_batch_size=batch_size,\n",
    "      save_strategy='epoch',\n",
    "      evaluation_strategy='epoch',\n",
    "      label_names=['labels'],\n",
    "      save_total_limit=2,\n",
    "      load_best_model_at_end=True,\n",
    "      predict_with_generate=True,\n",
    "      report_to=[],\n",
    "  )\n",
    "\n",
    "# Setup trainer.\n",
    "def get_trainer(model: transformers.AutoModelForCausalLM,\n",
    "    tokenizer: transformers.AutoTokenizer,\n",
    "    training_args: transformers.Seq2SeqTrainingArguments,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    ") -> transformers.Seq2SeqTrainer:\n",
    "  # customized compute metrics\n",
    "  def compute_metrics(eval_pred):\n",
    "    # print(\"Enter Evaluation\")\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE-N score\n",
    "    result = rouge_score(hypotheses=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # print(result)\n",
    "    return {\"rouge\": result}\n",
    "\n",
    "  # def data_collator(data):\n",
    "  #   passed_data = {'input_ids': torch.stack([f[0] for f in data]), 'attention_mask': torch.stack([f[1] for f in data]), 'labels': torch.stack([f[2] for f in data])}\n",
    "  #   print(passed_data)\n",
    "  #   x, y, z = passed_data['input_ids'].size()\n",
    "  #   print(x, y, z)\n",
    "  #   normalizer = torch.tensor(2048**0.5)\n",
    "  #   tmp = passed_data[\"input_ids\"] * normalizer\n",
    "  #   print(tmp.size())\n",
    "  #   return passed_data\n",
    "\n",
    "  data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors='pt', padding=\"max_length\", max_length=64)\n",
    "\n",
    "  return transformers.Seq2SeqTrainer(\n",
    "      model=model,\n",
    "      tokenizer=tokenizer,\n",
    "      args=training_args,\n",
    "      train_dataset=train_dataset,\n",
    "      # eval_dataset=eval_dataset,\n",
    "      # compute_metrics = compute_metrics,\n",
    "      # callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "      # data_collator = lambda data: {'input_ids': torch.stack([f[0] for f in data]), 'attention_mask': torch.stack([f[1] for f in data]), 'labels': torch.stack([f[2] for f in data])}\n",
    "      data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "height": 382
    },
    "executionInfo": {
     "elapsed": 2810,
     "status": "error",
     "timestamp": 1722373650995,
     "user": {
      "displayName": "Kai Zhang",
      "userId": "13781962467599818091"
     },
     "user_tz": 420
    },
    "id": "rs2BYw09x29G",
    "outputId": "8172fa87-77cb-4c38-a8d1-6d7ee86549e8"
   },
   "outputs": [
    {
     "debug": {
      "argv": [
       "/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.4240225017172.14b334fb3717c109/mount/server/kernel",
       "kernel",
       "-f",
       "/tmp/ipy-be-bsgiobmb/profile_colab/security/kernel-1ece04b2-5e47-4c34-97a6-97d52e4435c1.json",
       "--profile-dir",
       "/tmp/ipy-be-bsgiobmb/profile_colab",
       "--profile=colab",
       "--ipython-dir=/tmp/ipy-be-pmw9igvq",
       "--no-secure"
      ],
      "build": "Built on Mon Jul 29 07:36:15 2024 (1722263775)\nBuilt by coreml-pytorch-resources-colab@oxbda18.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/google3\nBuilt as //learning/pytorch/colab:kernel\nBuild ID: c12c6314-7e2c-41ba-81b0-8109851167e1\nBuilt from changelist 657179449 in a mint client based on //depot/google3\nBuild label: pytorch_colab_20240729_RC00\nBuild platform: gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8\nBuild tool: Blaze, release blaze-2024.07.24-1 (mainline @655309194)\nBuilt with par options [\"--compress\", \"--compress_level=6\"]\nCurrently running under Python 3.11.8: embedded.\n",
      "user": "kkaizh"
     },
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-eec5db87d576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# torch.cuda.empty_cache()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.4240225017172.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1858\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1859\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   1861\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.4240225017172.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2203\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2204\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2206\u001b[0m                 if (\n",
      "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.4240225017172.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3146\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3147\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3148\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.4240225017172.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.4240225017172.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             )\n\u001b[1;32m--> 525\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         )\n",
      "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.4240225017172.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m     _engine_run_backward(\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/export/hda3/borglet/remote_hdd_fs_dirs/0.colab_kernel_pytorch_gpu_kkaizh.kernel.kkaizh.4240225017172.14b334fb3717c109/mount/server/kernel.runfiles/google3/third_party/py/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.95 GiB. GPU "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class my_dataset(Dataset):\n",
    "  def __init__(self, raw_data, tokenizer, max_length):\n",
    "    self.raw_data = raw_data\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "    self.max_target_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.raw_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    data = self.raw_data[idx]\n",
    "    input_text = data['question']\n",
    "    target_text = data['answer']\n",
    "    input = self.tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "    # input_ids = torch.tensor(input[\"input_ids\"])\n",
    "    # attn_masks = torch.tensor(input[\"attention_mask\"])\n",
    "    input_ids = input[\"input_ids\"]\n",
    "    attn_masks = input[\"attention_mask\"]\n",
    "    target = self.tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=self.max_target_length)\n",
    "    # labels = torch.tensor(target[\"input_ids\"])\n",
    "    labels = target[\"input_ids\"]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attn_masks, \"labels\": labels}\n",
    "\n",
    "train_dataset = my_dataset(raw_train_data[:20], tokenizer, 64)\n",
    "eval_dataset = my_dataset(raw_train_data[50:60], tokenizer, 64)\n",
    "\n",
    "training_args = get_training_args(num_epochs=2, batch_size=1, learning_rate=0.001, weight_decay=0.2)\n",
    "trainer = get_trainer(model, tokenizer, training_args, train_dataset, eval_dataset)\n",
    "# torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//learning/pytorch/colab:kernel",
    "kind": "private"
   },
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1be5de668f4b469686aa7dcea6e7c1e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_269d23517eaf4ea788cb1905d087e245",
       "IPY_MODEL_c85c9115be104f638d0f8a3509eeb7af",
       "IPY_MODEL_2e8dabd840714dc5aa6430d4cc6b0183"
      ],
      "layout": "IPY_MODEL_4f43a9997cb4485fae12a3c735a27098"
     }
    },
    "1ea7f4804a2c45c9bb5395a3e4cd3c2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "269d23517eaf4ea788cb1905d087e245": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e19ccaeba1e2440e829c5c7b93d2ac23",
      "placeholder": "​",
      "style": "IPY_MODEL_1ea7f4804a2c45c9bb5395a3e4cd3c2f",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "2e8dabd840714dc5aa6430d4cc6b0183": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0b9b813e45e4dbf8ccf9a36f8777a53",
      "placeholder": "​",
      "style": "IPY_MODEL_5fee170c00434a1396169c5e55356ca1",
      "value": " 2/2 [00:14&lt;00:00,  5.85s/it]"
     }
    },
    "4f43a9997cb4485fae12a3c735a27098": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fee170c00434a1396169c5e55356ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a21b4a600bc24402a927e247088401c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd259eeb05fb4915b5294279770f605d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c85c9115be104f638d0f8a3509eeb7af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a21b4a600bc24402a927e247088401c7",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd259eeb05fb4915b5294279770f605d",
      "value": 2
     }
    },
    "d0b9b813e45e4dbf8ccf9a36f8777a53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e19ccaeba1e2440e829c5c7b93d2ac23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
